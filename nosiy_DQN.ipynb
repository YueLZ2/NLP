{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def distance(point1, point2):\n",
    "    return math.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "\n",
    "class Maze:\n",
    "    def __init__(self, args, size=40):\n",
    "        self.args = args\n",
    "        self.size = size\n",
    "        self.maze = np.zeros((size, size))\n",
    "        self.start_pos = (0, 0)  # 起点位置\n",
    "        self.goal_area = [(i, j) for i in range(30, 40) for j in range(30, 40)]\n",
    "        self.paved_area = []  # 记录走过的路径（终点内）\n",
    "        self.max_goal_distance = distance((1, 1), (30, 30))  # 距离终点最长的距离\n",
    "        self.goal_pos = self._reset_goal()  # 初始化终点位置\n",
    "        self.steps = 0\n",
    "        self.max_steps = 500\n",
    "        self.animation_set = []\n",
    "        self.entered_goal_area = False  # 标注首次进入\n",
    "        # History of agent's positions\n",
    "        self.position_history = []\n",
    "        # Dictionary to count occurrences of positions\n",
    "        self.position_counts = {}\n",
    "\n",
    "        # Initialize obstacles and cliffs\n",
    "        self._obstacles_and_cliffs()\n",
    "        self.guiding_points = [ (22, 14), (23, 14),\n",
    "                               (24, 14), (25, 14), (26, 14), (27, 14), (28, 15)]\n",
    "\n",
    "    def _reset_goal(self):\n",
    "        return random.choice(self.goal_area)\n",
    "\n",
    "    def _obstacles_and_cliffs(self):\n",
    "        obstacles = [(1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15),\n",
    "                     (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24), (1, 25),\n",
    "                     (10, 3), (11, 4), (12, 5), (13, 6), (14, 7), (15, 8), (16, 9), (17, 10), (18, 11), (19, 12),\n",
    "                     (20, 13), (23, 4), (24, 4), (25, 4), (26, 4), (27, 4), (28, 4), (29, 4),\n",
    "                     (30, 4), (31, 4), (32, 4), (33, 4), (34, 4), (35, 4), (36, 4),\n",
    "                     (25, 22), (25, 23), (25, 24), (25, 25), (25, 26), (25, 27), (25, 28), (25, 29), (25, 30),\n",
    "                     (25, 31), (25, 32), (25, 33), (25, 34), (25, 35), (25, 36), (25, 37), (25, 38), (25, 39), ]\n",
    "        cliffs = [(2, 0), (2, 1),\n",
    "                  (22, 2), (22, 3), (22, 4), (22, 5), (22, 6), (22, 7), (22, 8),\n",
    "                  (24, 15), (24, 16), (24, 17), (24, 18), (24, 19), (24, 20), (24, 21),\n",
    "                  (36, 25), (37, 25), (38, 25), (39, 25), ]\n",
    "\n",
    "        guiding_points = [ ]\n",
    "\n",
    "        for obs in obstacles:\n",
    "            self.maze[obs] = -1  # obstacle\n",
    "\n",
    "        for clf in cliffs:\n",
    "            self.maze[clf] = -2  # cliff\n",
    "\n",
    "        for gui in guiding_points:\n",
    "            self.maze[gui] = -3\n",
    "\n",
    "    def reset(self):\n",
    "        self.animation_set = []  # 动画集合重置\n",
    "        self.paved_area = []  # 走过路径重置\n",
    "        self.entered_goal_area = False  # 是否进入终点区域重置\n",
    "        self.agent_pos = self.start_pos\n",
    "        self.steps = 0\n",
    "        self.goal_pos = self._reset_goal()\n",
    "        self.position_history = []\n",
    "        self.position_counts = {}\n",
    "        self.goal_reached = False  # 重置标志\n",
    "        return self.agent_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        # move\n",
    "        x, y = self.agent_pos\n",
    "\n",
    "        if action == 0:  # up\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # down\n",
    "            x = min(self.size - 1, x + 1)\n",
    "        elif action == 2:  # left\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # right\n",
    "            y = min(self.size - 1, y + 1)\n",
    "\n",
    "        # Update position history\n",
    "        current_position = self.agent_pos\n",
    "        self.position_history.append(current_position)\n",
    "        if len(self.position_history) > 100:\n",
    "            self.position_history.pop(0)\n",
    "\n",
    "        self.steps += 1\n",
    "        if not self.entered_goal_area:\n",
    "            # reward = -1\n",
    "            reward = -5 * (distance((x, y), (30, 30)) / self.max_goal_distance)  # 距离终点越近，惩罚越小\n",
    "            if current_position in self.position_counts:\n",
    "                self.position_counts[current_position] += 1\n",
    "            else:\n",
    "                self.position_counts[current_position] = 1\n",
    "\n",
    "            # 检查重复数\n",
    "            if self.position_counts[current_position] >= 3:\n",
    "                reward += -20  # 重复超过3给予更大的负奖励\n",
    "        else:\n",
    "            if (x, y) in self.paved_area:\n",
    "                reward = -5  # 走了走过的路，小惩罚\n",
    "            elif (x, y) not in self.goal_area:\n",
    "                reward = -20  # 走出去了，大惩罚\n",
    "            else:\n",
    "                reward = -1  # 积极探索，奖励\n",
    "                self.paved_area.append((x, y))\n",
    "\n",
    "        # # 检查引导点\n",
    "        # if (x, y) in self.guiding_points:\n",
    "        #     reward += 15  # Increase reward for reaching a guiding point\n",
    "        #     self.guiding_points.remove((x, y))  # Remove the guiding point once reached\n",
    "\n",
    "        if action == 1:         # 向下\n",
    "            if self.size - 6 >= x:\n",
    "                reward += 15\n",
    "            else:\n",
    "                reward += 0\n",
    "        elif action == 3:\n",
    "            if x <= 26 and x > 22:\n",
    "                reward +=0\n",
    "            else:\n",
    "                reward += 5\n",
    "        elif action == 0:\n",
    "            if self.size - 6 >= x:\n",
    "                reward += -2\n",
    "            else:\n",
    "                reward += 0\n",
    "        # 检查是否遇到障碍\n",
    "        if self.maze[x, y] == -1:\n",
    "            reward += -8\n",
    "            return self.agent_pos, reward, False  # meet obstacle\n",
    "\n",
    "        # 是否掉入悬崖\n",
    "        if self.maze[x, y] == -2:\n",
    "            reward += -80  # 死了，给个大惩罚\n",
    "            return (x, y), reward, True  # fell into cliff\n",
    "\n",
    "        self.agent_pos = (x, y)\n",
    "\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward += 1000  # 到终点的大大奖励\n",
    "            self.goal_reached = True  # 抵达终点\n",
    "            done = True\n",
    "        elif self.agent_pos in self.goal_area:\n",
    "            # 如果还没有进入过目标区域，给予一次奖励并设置标志\n",
    "            if not self.entered_goal_area:\n",
    "                reward += 500  # 阶段性大奖励\n",
    "                self.entered_goal_area = True\n",
    "            done = False\n",
    "        elif self.steps >= self.max_steps:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        if self.steps % 20 == 0:\n",
    "            self.goal_pos = self._reset_goal()\n",
    "            self.paved_area = []\n",
    "\n",
    "        return self.agent_pos, reward, done\n",
    "\n",
    "    def show(self):\n",
    "        plt.clf()\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.set_xlim(0, self.size)\n",
    "        ax.set_ylim(0, self.size)\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if self.maze[i, j] == -1:\n",
    "                    rect = patches.Rectangle((j, self.size - i - 1), 1, 1,\n",
    "                                             linewidth=1, edgecolor='black', facecolor='grey')\n",
    "                    ax.add_patch(rect)\n",
    "                elif self.maze[i, j] == -2:\n",
    "                    rect = patches.Rectangle((j, self.size - i - 1), 1, 1,\n",
    "                                             linewidth=1, edgecolor='black', facecolor='red')\n",
    "                    ax.add_patch(rect)\n",
    "                elif self.maze[i, j] == -3:\n",
    "                    rect = patches.Rectangle((j, self.size - i - 1), 1, 1,\n",
    "                                             linewidth=1, edgecolor='black', facecolor='violet')\n",
    "                    ax.add_patch(rect)\n",
    "\n",
    "        rect = patches.Rectangle((self.start_pos[1], self.size - self.start_pos[0] - 1), 1, 1,\n",
    "                                 linewidth=1, edgecolor='black', facecolor='yellow')\n",
    "        ax.add_patch(rect)\n",
    "        for pos in self.goal_area:\n",
    "            if pos == self.goal_pos:\n",
    "                rect = patches.Rectangle((pos[1], self.size - pos[0] - 1), 1, 1,\n",
    "                                         linewidth=1, edgecolor='black',\n",
    "                                         facecolor='green')\n",
    "            else:\n",
    "                rect = patches.Rectangle((pos[1], self.size - pos[0] - 1), 1, 1,\n",
    "                                         linewidth=1, edgecolor='black',\n",
    "                                         facecolor='orange')\n",
    "            ax.add_patch(rect)\n",
    "        rect = patches.Rectangle((self.agent_pos[1], self.size - self.agent_pos[0] - 1), 1, 1,\n",
    "                                 linewidth=1, edgecolor='black', facecolor='blue')\n",
    "        ax.add_patch(rect)\n",
    "        fig.savefig('./temp/temp.png')\n",
    "        image = cv2.imread('./temp/temp.png')\n",
    "        self.animation_set.append(image)\n",
    "        print(f\"Added frame {len(self.animation_set)} to animation_set\")  # 调试信息\n",
    "        plt.close(fig)\n",
    "\n",
    "    def show_animation(self, name):\n",
    "        # 保存动画\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        video = cv2.VideoWriter(os.path.join(self.args.save_dir, name), fourcc, self.args.fps, (1000, 1000))\n",
    "        for img in self.animation_set:\n",
    "            video.write(img)\n",
    "        video.release()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    '''From https://github.com/Lizhi-sjtu/DRL-code-pytorch/blob/main/3.Rainbow_DQN/network.py'''\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigma_init = sigma_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
    "\n",
    "        self.reset_parameters() # for mu and sigma\n",
    "        self.reset_noise() # for epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.reset_noise()\n",
    "            weight = self.weight_mu + self.weight_sigma.mul(self.weight_epsilon)  # mul是对应元素相乘\n",
    "            bias = self.bias_mu + self.bias_sigma.mul(self.bias_epsilon)\n",
    "\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "\n",
    "        return F.linear(x, weight, bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "\n",
    "        self.weight_sigma.data.fill_(self.sigma_init / math.sqrt(self.in_features))\n",
    "        self.bias_sigma.data.fill_(self.sigma_init / math.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_i = self.scale_noise(self.in_features)\n",
    "        epsilon_j = self.scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(torch.ger(epsilon_j, epsilon_i))\n",
    "        self.bias_epsilon.copy_(epsilon_j)\n",
    "\n",
    "    def scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        x = x.sign().mul(x.abs().sqrt())\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def make_maze_env():\n",
    "    env = Maze(args)\n",
    "    return env"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# 构建网络\n",
    "def build_net(layer_shape, activation, output_activation):\n",
    "    layers = []\n",
    "    for j in range(len(layer_shape) - 1):\n",
    "        if j < len(layer_shape) - 2:\n",
    "            layers += [nn.Linear(layer_shape[j], layer_shape[j + 1]), activation()]\n",
    "        else:\n",
    "            layers += [NoisyLinear(layer_shape[j], layer_shape[j + 1], sigma_init=0.25), output_activation()]\n",
    "    return nn.Sequential(*layers)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, device, max_size=int(1e6)):\n",
    "        self.max_size = max_size\n",
    "        self.device = device\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.s = torch.zeros((max_size, state_dim), dtype=torch.float, device=device)\n",
    "        self.a = torch.zeros((max_size, 1), dtype=torch.long, device=device)\n",
    "        self.r = torch.zeros((max_size, 1), dtype=torch.float, device=device)\n",
    "        self.s_next = torch.zeros((max_size, state_dim), dtype=torch.float, device=device)\n",
    "        self.dw = torch.zeros((max_size, 1), dtype=torch.bool, device=device)\n",
    "\n",
    "    def add(self, s, a, r, s_next, dw):\n",
    "        self.s[self.ptr] = torch.FloatTensor(s).to(self.device)\n",
    "        self.a[self.ptr] = torch.LongTensor([a]).to(self.device)\n",
    "        self.r[self.ptr] = torch.FloatTensor([r]).to(self.device)\n",
    "        self.s_next[self.ptr] = torch.FloatTensor(s_next).to(self.device)\n",
    "        self.dw[self.ptr] = torch.BoolTensor([dw]).to(self.device)\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = torch.randint(0, self.size, size=(batch_size,), device=self.device)\n",
    "        return self.s[ind], self.a[ind], self.r[ind], self.s_next[ind], self.dw[ind]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "class Noisy_Q_Net(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hid_shape):\n",
    "        super(Noisy_Q_Net, self).__init__()\n",
    "        layers = [state_dim] + list(hid_shape) + [action_dim]\n",
    "        self.Q = build_net(layers, nn.ReLU, nn.Identity)\n",
    "\n",
    "    def forward(self, s):\n",
    "        q = self.Q(s)\n",
    "        return q\n",
    "\n",
    "class NoisyNetDQN_agent:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.tau = 0.005\n",
    "        self.replay_buffer = ReplayBuffer(self.state_dim, self.device, max_size=self.buffer_size)\n",
    "        self.q_net = Noisy_Q_Net(self.state_dim, self.action_dim, (self.net_width, self.net_width)).to(self.device)\n",
    "        self.q_net_optimizer = optim.Adam(self.q_net.parameters(), lr=self.lr)\n",
    "        self.q_target = copy.deepcopy(self.q_net)\n",
    "        for p in self.q_target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            if isinstance(state, tuple):\n",
    "                state = np.array(state)\n",
    "            state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "            action = self.q_net(state).argmax().item()\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "        s, a, r, s_next, dw = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            max_q_next = self.q_target(s_next).max(1)[0].unsqueeze(1)\n",
    "            target_Q = r + (~dw) * self.gamma * max_q_next\n",
    "\n",
    "        current_q = self.q_net(s).gather(1, a)\n",
    "        q_loss = F.mse_loss(current_q, target_Q)\n",
    "\n",
    "        self.q_net_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_net_optimizer.step()\n",
    "\n",
    "        for param, target_param in zip(self.q_net.parameters(), self.q_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def save(self, algo, steps):\n",
    "        print(f\"algo:{algo}steps:{steps}\")\n",
    "        torch.save(self.q_net.state_dict(), f\"./{args.save_dir}/{algo}_{steps}k.pth\")\n",
    "        print(f\"模型已保存在{args.save_dir}/{algo}_{steps}\")\n",
    "\n",
    "    def load(self, algo, steps):\n",
    "        self.q_net.load_state_dict(torch.load(f\"./{args.save_dir}/{algo}_{steps}k.pth\", map_location=self.device))\n",
    "        self.q_target.load_state_dict(torch.load(f\"./{args.save_dir}/{algo}_{steps}k.pth\", map_location=self.device))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "class Args:\n",
    "    args = type('', (), {})()\n",
    "    fps = 30\n",
    "    num_steps = 5000\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lr = 1e-3\n",
    "    memory_size = 1000\n",
    "    batch_size = 64\n",
    "    gamma = 0.99\n",
    "    epsilon = 1.0\n",
    "    net_width = 128  # 网络宽度\n",
    "    target_update = 10\n",
    "    update_target_every = 1000\n",
    "    save_dir = \"./model_w/Noisy_DQN\"\n",
    "    load_model = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Reward: -13904.470353364728, Episode Length: 806\n",
      "Episode Reward: -60.089909179475455, Episode Length: 2\n",
      "Episode Reward: -60.089909179475455, Episode Length: 2\n",
      "Episode Reward: -60.089909179475455, Episode Length: 2\n",
      "Episode Reward: -60.089909179475455, Episode Length: 2\n",
      "Episode Reward: -60.089909179475455, Episode Length: 2\n",
      "Episode Reward: -60.089909179475455, Episode Length: 2\n",
      "Episode Reward: -60.001486657127984, Episode Length: 3\n",
      "Episode Reward: -60.001486657127984, Episode Length: 3\n",
      "Episode Reward: -60.001486657127984, Episode Length: 3\n",
      "Episode Reward: -60.001486657127984, Episode Length: 3\n",
      "Episode Reward: -60.001486657127984, Episode Length: 3\n",
      "Episode Reward: -60.001486657127984, Episode Length: 3\n",
      "Episode Reward: -60.001486657127984, Episode Length: 3\n",
      "Episode Reward: -6163.199541275671, Episode Length: 502\n",
      "Episode Reward: -8383.33844028067, Episode Length: 500\n",
      "Episode Reward: -20380.992636310326, Episode Length: 1397\n",
      "Episode Reward: -6231.643645310853, Episode Length: 500\n",
      "Episode Reward: -6432.322483242746, Episode Length: 516\n",
      "111111\n",
      "algo:NoisyNetDQNsteps:4999\n",
      "模型已保存在./model_w/Noisy_DQN/NoisyNetDQN_4999\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = Args()\n",
    "    env = make_maze_env()\n",
    "\n",
    "\n",
    "    agent = NoisyNetDQN_agent(\n",
    "        state_dim= 2,\n",
    "        action_dim= 4,\n",
    "        buffer_size=args.memory_size,\n",
    "        net_width=args.net_width,\n",
    "        lr=args.lr,\n",
    "        gamma=0.99,\n",
    "        batch_size=args.batch_size,\n",
    "        device=args.device\n",
    "    )\n",
    "\n",
    "    # if args.load_model is not None:\n",
    "    #     agent.load(args.load_model)\n",
    "\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    for t in range(args.num_steps):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            print(f\"Episode Reward: {episode_reward}, Episode Length: {episode_length}\")\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "\n",
    "        if t > args.batch_size:\n",
    "            agent.train()\n",
    "        if t+1 == args.num_steps :\n",
    "            print(\"111111\")\n",
    "            agent.save(\"NoisyNetDQN\", t )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added frame 1 to animation_set\n",
      "Added frame 2 to animation_set\n",
      "Added frame 3 to animation_set\n",
      "Added frame 4 to animation_set\n",
      "Added frame 5 to animation_set\n",
      "Added frame 6 to animation_set\n",
      "Added frame 7 to animation_set\n",
      "Added frame 8 to animation_set\n",
      "Added frame 9 to animation_set\n",
      "Added frame 10 to animation_set\n",
      "Added frame 11 to animation_set\n",
      "Added frame 12 to animation_set\n",
      "Added frame 13 to animation_set\n",
      "Added frame 14 to animation_set\n",
      "Added frame 15 to animation_set\n",
      "Added frame 16 to animation_set\n",
      "Added frame 17 to animation_set\n",
      "Added frame 18 to animation_set\n",
      "Added frame 19 to animation_set\n",
      "Added frame 20 to animation_set\n",
      "Added frame 21 to animation_set\n",
      "Added frame 22 to animation_set\n",
      "Added frame 23 to animation_set\n",
      "Added frame 24 to animation_set\n",
      "Added frame 25 to animation_set\n",
      "Added frame 26 to animation_set\n",
      "Added frame 27 to animation_set\n",
      "Added frame 28 to animation_set\n",
      "Added frame 29 to animation_set\n",
      "Added frame 30 to animation_set\n",
      "Added frame 31 to animation_set\n",
      "Added frame 32 to animation_set\n",
      "Added frame 33 to animation_set\n",
      "Added frame 34 to animation_set\n",
      "Added frame 35 to animation_set\n",
      "Added frame 36 to animation_set\n",
      "Added frame 37 to animation_set\n",
      "Added frame 38 to animation_set\n",
      "Added frame 39 to animation_set\n",
      "Added frame 40 to animation_set\n",
      "Added frame 41 to animation_set\n",
      "Added frame 42 to animation_set\n",
      "Added frame 43 to animation_set\n",
      "Added frame 44 to animation_set\n",
      "Added frame 45 to animation_set\n",
      "Added frame 46 to animation_set\n",
      "Added frame 47 to animation_set\n",
      "Added frame 48 to animation_set\n",
      "Added frame 49 to animation_set\n",
      "Added frame 50 to animation_set\n",
      "Added frame 51 to animation_set\n",
      "Added frame 52 to animation_set\n",
      "Added frame 53 to animation_set\n",
      "Added frame 54 to animation_set\n",
      "Added frame 55 to animation_set\n",
      "Added frame 56 to animation_set\n",
      "Added frame 57 to animation_set\n",
      "Added frame 58 to animation_set\n",
      "Added frame 59 to animation_set\n",
      "Added frame 60 to animation_set\n",
      "Added frame 61 to animation_set\n",
      "Added frame 62 to animation_set\n",
      "Added frame 63 to animation_set\n",
      "Added frame 64 to animation_set\n",
      "Added frame 65 to animation_set\n",
      "Added frame 66 to animation_set\n",
      "Added frame 67 to animation_set\n",
      "Added frame 68 to animation_set\n",
      "Added frame 69 to animation_set\n",
      "Added frame 70 to animation_set\n",
      "Added frame 71 to animation_set\n",
      "Added frame 72 to animation_set\n",
      "Added frame 73 to animation_set\n",
      "Added frame 74 to animation_set\n",
      "Added frame 75 to animation_set\n",
      "Added frame 76 to animation_set\n",
      "Added frame 77 to animation_set\n",
      "Added frame 78 to animation_set\n",
      "Added frame 79 to animation_set\n",
      "Added frame 80 to animation_set\n",
      "Added frame 81 to animation_set\n",
      "Added frame 82 to animation_set\n",
      "Added frame 83 to animation_set\n",
      "Added frame 84 to animation_set\n",
      "Added frame 85 to animation_set\n",
      "Added frame 86 to animation_set\n",
      "Added frame 87 to animation_set\n",
      "Added frame 88 to animation_set\n",
      "Added frame 89 to animation_set\n",
      "Added frame 90 to animation_set\n",
      "Added frame 91 to animation_set\n",
      "Added frame 92 to animation_set\n",
      "Added frame 93 to animation_set\n",
      "Added frame 94 to animation_set\n",
      "Added frame 95 to animation_set\n",
      "Added frame 96 to animation_set\n",
      "Added frame 97 to animation_set\n",
      "Added frame 98 to animation_set\n",
      "Added frame 99 to animation_set\n",
      "Added frame 100 to animation_set\n",
      "Added frame 101 to animation_set\n",
      "Added frame 102 to animation_set\n",
      "Added frame 103 to animation_set\n",
      "Added frame 104 to animation_set\n",
      "Added frame 105 to animation_set\n",
      "Added frame 106 to animation_set\n",
      "Added frame 107 to animation_set\n",
      "Added frame 108 to animation_set\n",
      "Added frame 109 to animation_set\n",
      "Added frame 110 to animation_set\n",
      "Added frame 111 to animation_set\n",
      "Added frame 112 to animation_set\n",
      "Added frame 113 to animation_set\n",
      "Added frame 114 to animation_set\n",
      "Added frame 115 to animation_set\n",
      "Added frame 116 to animation_set\n",
      "Added frame 117 to animation_set\n",
      "Added frame 118 to animation_set\n",
      "Added frame 119 to animation_set\n",
      "Added frame 120 to animation_set\n",
      "Added frame 121 to animation_set\n",
      "Added frame 122 to animation_set\n",
      "Added frame 123 to animation_set\n",
      "Added frame 124 to animation_set\n",
      "Added frame 125 to animation_set\n",
      "Added frame 126 to animation_set\n",
      "Added frame 127 to animation_set\n",
      "Added frame 128 to animation_set\n",
      "Added frame 129 to animation_set\n",
      "Added frame 130 to animation_set\n",
      "Added frame 131 to animation_set\n",
      "Added frame 132 to animation_set\n",
      "Added frame 133 to animation_set\n",
      "Added frame 134 to animation_set\n",
      "Added frame 135 to animation_set\n",
      "Added frame 136 to animation_set\n",
      "Added frame 137 to animation_set\n",
      "Added frame 138 to animation_set\n",
      "Added frame 139 to animation_set\n",
      "Added frame 140 to animation_set\n",
      "Added frame 141 to animation_set\n",
      "Added frame 142 to animation_set\n",
      "Added frame 143 to animation_set\n",
      "Added frame 144 to animation_set\n",
      "Added frame 145 to animation_set\n",
      "Added frame 146 to animation_set\n",
      "Added frame 147 to animation_set\n",
      "Added frame 148 to animation_set\n",
      "Added frame 149 to animation_set\n",
      "Added frame 150 to animation_set\n",
      "Added frame 151 to animation_set\n",
      "Added frame 152 to animation_set\n",
      "Added frame 153 to animation_set\n",
      "Added frame 154 to animation_set\n",
      "Added frame 155 to animation_set\n",
      "Added frame 156 to animation_set\n",
      "Added frame 157 to animation_set\n",
      "Added frame 158 to animation_set\n",
      "Added frame 159 to animation_set\n",
      "Added frame 160 to animation_set\n",
      "Added frame 161 to animation_set\n",
      "Added frame 162 to animation_set\n",
      "Added frame 163 to animation_set\n",
      "Added frame 164 to animation_set\n",
      "Added frame 165 to animation_set\n",
      "Added frame 166 to animation_set\n",
      "Added frame 167 to animation_set\n",
      "Added frame 168 to animation_set\n",
      "Added frame 169 to animation_set\n",
      "Added frame 170 to animation_set\n",
      "Added frame 171 to animation_set\n",
      "Added frame 172 to animation_set\n",
      "Added frame 173 to animation_set\n",
      "Added frame 174 to animation_set\n",
      "Added frame 175 to animation_set\n",
      "Added frame 176 to animation_set\n",
      "Added frame 177 to animation_set\n",
      "Added frame 178 to animation_set\n",
      "Added frame 179 to animation_set\n",
      "Added frame 180 to animation_set\n",
      "Added frame 181 to animation_set\n",
      "Added frame 182 to animation_set\n",
      "Added frame 183 to animation_set\n",
      "Added frame 184 to animation_set\n",
      "Added frame 185 to animation_set\n",
      "Added frame 186 to animation_set\n",
      "Added frame 187 to animation_set\n",
      "Added frame 188 to animation_set\n",
      "Added frame 189 to animation_set\n",
      "Added frame 190 to animation_set\n",
      "Added frame 191 to animation_set\n",
      "Added frame 192 to animation_set\n",
      "Added frame 193 to animation_set\n",
      "Added frame 194 to animation_set\n",
      "Added frame 195 to animation_set\n",
      "Added frame 196 to animation_set\n",
      "Added frame 197 to animation_set\n",
      "Added frame 198 to animation_set\n",
      "Added frame 199 to animation_set\n",
      "Added frame 200 to animation_set\n",
      "Video saved as test_animation.avi\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Args()\n",
    "env = make_maze_env()\n",
    "\n",
    "\n",
    "agent = NoisyNetDQN_agent(\n",
    "    state_dim= 2,\n",
    "    action_dim= 4,\n",
    "    buffer_size=args.memory_size,\n",
    "    net_width=args.net_width,\n",
    "    lr=args.lr,\n",
    "    gamma=0.99,\n",
    "    batch_size=args.batch_size,\n",
    "    device=args.device\n",
    ")\n",
    "\n",
    "def test_noisynet_dqn(agent, env, num_episodes=1):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "\n",
    "        for t in range(100):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            env.show()\n",
    "            if done:\n",
    "                break\n",
    "        env.show_animation('test_animation.avi')\n",
    "        print('Video saved as test_animation.avi')\n",
    "\n",
    "\n",
    "model_path = args.save_dir+\"NoisyNetDQN_24999.pth\"  # 替换为实际模型文件路径\n",
    "agent.load(\"NoisyNetDQN\", '4999')\n",
    "\n",
    "# 运行测试\n",
    "test_noisynet_dqn(agent, env, num_episodes=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
